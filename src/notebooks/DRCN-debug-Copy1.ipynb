{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed5498ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "from argparse import Namespace\n",
    "import tqdm\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61afbad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GLOBALS \n",
    "SOURCE_GENOME=\"mm10\"\n",
    "TF=\"Hnf4a\"\n",
    "SOURCE_GENOME_FASTA='../../genomes/mm10_no_alt_analysis_set_ENCODE.fasta'\n",
    "TARGET_GENOME = \"hg38\"\n",
    "TARGET_GENOME_FASTA = \"../../genomes/GRCh38_no_alt_analysis_set_GCA_000001405.15.fasta\"\n",
    "PILOT_STUDY=False\n",
    "MODEL_NAME=\"drcn_debug\"\n",
    "PYTORCH_DEVICE=\"cuda\"\n",
    "TRAIN=True\n",
    "MODEL_STORAGE_SUFFIX=\"_pilot\" if PILOT_STUDY else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1d48cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../\")\n",
    "from utils import datasets,samplers,models,utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "252f361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logger config\n",
    "logging.basicConfig(filename=f'./log/{TF}_{MODEL_NAME}{MODEL_STORAGE_SUFFIX}.log', filemode='w', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507ef799",
   "metadata": {},
   "source": [
    "# Define namespace arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4d07c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\t../../torch_models/mm10/Hnf4a/drcn_debug/drcn_debug.pth\n",
      "Using CUDA: True\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path information\n",
    "    model_state_file=f'{MODEL_NAME}{MODEL_STORAGE_SUFFIX}.pth',\n",
    "    source_csv=f'../../data/{SOURCE_GENOME}/{TF}/split_data.csv.gz',\n",
    "    source_genome_fasta=SOURCE_GENOME_FASTA,\n",
    "    target_csv = f'../../data/{TARGET_GENOME}/{TF}/split_data.csv.gz',\n",
    "    target_genome_fasta = TARGET_GENOME_FASTA,\n",
    "    model_save_dir=f'../../torch_models/{SOURCE_GENOME}/{TF}/{MODEL_NAME}/',\n",
    "    results_save_dir=f'../../results/{SOURCE_GENOME}/{TF}/',\n",
    "    feat_size=(4, 500),\n",
    "    \n",
    "    # Model hyper parameters\n",
    "    conv_filters=240,\n",
    "    conv_kernelsize=20,\n",
    "    maxpool_strides=15,\n",
    "    maxpool_size=15,\n",
    "    lstm_outnodes=32,\n",
    "    linear1_nodes=1024,\n",
    "    dropout_prob=0.5,\n",
    "    \n",
    "    # Training hyper parameters\n",
    "    batch_size=128,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.0001,\n",
    "    num_epochs=50,\n",
    "    tolerance=1e-3,\n",
    "    seed=1337,\n",
    "    \n",
    "    # Runtime options\n",
    "    catch_keyboard_interrupt=True,\n",
    "    cuda=True if PYTORCH_DEVICE==\"cuda\" else False,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    pilot=PILOT_STUDY, # 2% of original dataset\n",
    "    train=TRAIN,\n",
    "    test_batch_size=int(2e3)\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "\n",
    "    args.model_state_file = os.path.join(args.model_save_dir,\n",
    "                                         args.model_state_file)\n",
    "    \n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "    \n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "utils.set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "utils.handle_dirs(args.model_save_dir)\n",
    "utils.handle_dirs(args.results_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5db264",
   "metadata": {},
   "source": [
    "# DRCN Classifier-Reconstructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62a6bcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Conventional model with reconstruction\n",
    "\n",
    "class encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        super(encoder, self).__init__()\n",
    "        self.featurizer=models.TFCNN(channels=args.feat_size[0], \n",
    "                             conv_filters=args.conv_filters, \n",
    "                             conv_kernelsize=args.conv_kernelsize, \n",
    "                             maxpool_size=args.maxpool_size, \n",
    "                             maxpool_strides=args.maxpool_strides)\n",
    "        \n",
    "    def forward(self, x_in):\n",
    "        x_in = self.featurizer(x_in)\n",
    "        return x_in\n",
    "    \n",
    "class decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        super(decoder, self).__init__()\n",
    "        self.dconv0 = nn.ConvTranspose1d(args.conv_filters, 120, kernel_size=4, stride=1)\n",
    "        self.bn0 = nn.BatchNorm1d(120, affine=False)\n",
    "        self.prelu0 = nn.PReLU()\n",
    "        self.dconv1 = nn.ConvTranspose1d(120, 64, kernel_size=3, stride=2, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(64, affine=False)\n",
    "        self.prelu1 = nn.PReLU()\n",
    "        self.dconv2 = nn.ConvTranspose1d(64,32, kernel_size=3, stride=2, padding=4)\n",
    "        self.bn2 = nn.BatchNorm1d(32, affine=False)\n",
    "        self.prelu2 = nn.PReLU()\n",
    "        self.dconv3 = nn.ConvTranspose1d(32, 16, kernel_size=3, stride=2, padding=4)\n",
    "        self.bn3 = nn.BatchNorm1d(16, affine=False)\n",
    "        self.prelu3 = nn.PReLU()\n",
    "        self.dconv4 = nn.ConvTranspose1d(16, 8, kernel_size=3, stride=2, padding=4)\n",
    "        self.bn4 = nn.BatchNorm1d(8, affine=False)\n",
    "        self.prelu4 = nn.PReLU()\n",
    "        self.dconv5 = nn.ConvTranspose1d(8, 4, kernel_size=2, stride=1, padding=2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.prelu0(self.bn0(self.dconv0(x)))\n",
    "        x = self.prelu1(self.bn1(self.dconv1(x)))\n",
    "        x = self.prelu2(self.bn2(self.dconv2(x)))\n",
    "        x = self.prelu3(self.bn3(self.dconv3(x)))\n",
    "        x = self.prelu4(self.bn4(self.dconv4(x)))\n",
    "        x = self.softmax(self.dconv5(x))\n",
    "        return x\n",
    "\n",
    "class DRCN(nn.Module):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        super(DRCN, self).__init__()\n",
    "        self.featurizer=encoder(args)\n",
    "        self.classifier=models.TFLSTM(input_features=args.conv_filters, lstm_nodes=args.lstm_outnodes, \n",
    "                               fc1_nodes=args.linear1_nodes)\n",
    "        \n",
    "        self.decoder=decoder(args)\n",
    "        \n",
    "    def forward(self, x_in, apply_sigmoid=False):\n",
    "        x_in = self.featurizer(x_in)\n",
    "        class_out = self.classifier(x_in, apply_sigmoid=apply_sigmoid)\n",
    "        return class_out\n",
    "    \n",
    "    def reconstruct(self, x):\n",
    "        x = self.featurizer(x)\n",
    "        recon_x = self.decoder(x)\n",
    "        return recon_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4430484",
   "metadata": {},
   "source": [
    "## Load classifier from conventional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b61338bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_drcn_from_hybrid(drcn, hybrid_path):\n",
    "    hybrid_state_dict = torch.load(hybrid_path)\n",
    "    \n",
    "    pretrained_dict = {}\n",
    "\n",
    "    for k,v in hybrid_state_dict.items():\n",
    "        if k.startswith(\"featurizer\"):\n",
    "            newk = \"featurizer.\" + k\n",
    "            pretrained_dict[newk] = v\n",
    "        else:\n",
    "            pretrained_dict[k] = v\n",
    "    \n",
    "    drcn.load_state_dict(pretrained_dict, strict=False)\n",
    "    \n",
    "    return drcn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bff15f",
   "metadata": {},
   "source": [
    "# DRCN Training Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96df60a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_requires_grad(model, requires_grad=True):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad=requires_grad\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8439a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_drcn(args):\n",
    "    \n",
    "    # Load the dataset\n",
    "    logging.debug(f'Loading source and target data...')\n",
    "    src_dataset, tgt_dataset = datasets.load_data(args)\n",
    "    \n",
    "    # Initializing models\n",
    "    logging.debug(f'Initializing model...')\n",
    "    classifier = DRCN(args)\n",
    "    logging.debug(classifier)\n",
    "    \n",
    "    hybrid_path = args.model_state_file.replace(MODEL_NAME, \"hybrid\")\n",
    "    classifier = load_drcn_from_hybrid(classifier, hybrid_path)\n",
    "    \n",
    "    classifier.to(args.device)\n",
    "    model_params = utils.get_n_params(classifier)\n",
    "    logging.debug(f\"The model has {model_params} parameters.\")\n",
    "        \n",
    "    # Defining loss functions, optimizers\n",
    "    bce_loss_func = nn.BCEWithLogitsLoss()\n",
    "    mse_loss_func = nn.MSELoss()\n",
    "    opt = optim.Adam(classifier.parameters(), lr=args.learning_rate, eps=1e-7)\n",
    "    \n",
    "    logging.debug(\"Making samplers...\")\n",
    "    # weighted train and unweighted valid samplers for classifier part of the model\n",
    "    src_dataset.set_split(\"train\")\n",
    "    train_sampler = samplers.get_sampler(src_dataset, weighted=True, mini=False)\n",
    "    nsamples = train_sampler.num_samples\n",
    "    \n",
    "#     src_dataset.set_split(\"valid\")    \n",
    "#     valid_sampler = samplers.get_sampler(dataset, weighted=False, mini=True)\n",
    "    \n",
    "    \n",
    "    # unweighted sampler of target data for reconstruction and classification loss validation\n",
    "    tgt_dataset.set_split(\"valid\")\n",
    "    tgt_valid_sampler = samplers.get_sampler(tgt_dataset, weighted=False, mini=False)\n",
    "    \n",
    "    # unweighted sampler of target data for reconstruction loss \n",
    "    tgt_dataset.set_split(\"train\")\n",
    "    tgt_sampler = samplers.get_sampler(tgt_dataset, weighted=False, mini=False)\n",
    "    \n",
    "    # Defining initial train state\n",
    "    train_state = utils.make_train_state(args)\n",
    "    \n",
    "    # tqdm progress bar initialize\n",
    "    epoch_bar = tqdm.notebook.tqdm(desc='training routine', \n",
    "                          total=args.num_epochs,\n",
    "                          position=0)\n",
    "    \n",
    "    train_bar = tqdm.notebook.tqdm(desc=f'split=train',\n",
    "                              total=nsamples//int(args.batch_size), \n",
    "                              position=1, \n",
    "                              leave=True)\n",
    "    \n",
    "    tgt_dataset.set_split('valid') # tgt data\n",
    "    val_bar = tqdm.notebook.tqdm(desc='split=valid',\n",
    "                        total=len(tgt_dataset)//int(args.batch_size*1e1), \n",
    "                        position=2, \n",
    "                        leave=True)\n",
    "    \n",
    "    ##### Training Routine #####\n",
    "    \n",
    "    try:\n",
    "        for epoch_index in range(args.num_epochs):\n",
    "            train_state['epoch_index'] = epoch_index\n",
    "\n",
    "            # Iterate over training dataset\n",
    "\n",
    "            # setup: batch generator (w), tgt_batch_generator (uw)\n",
    "            # set loss and acc to 0, \n",
    "            # set train mode on\n",
    "            src_dataset.set_split('train')\n",
    "            batch_generator = utils.generate_batches(src_dataset, sampler=train_sampler,\n",
    "                                               batch_size=int(args.batch_size), \n",
    "                                               device=args.device)\n",
    "\n",
    "            tgt_dataset.set_split('train')\n",
    "            tgt_batch_generator = utils.generate_batches(tgt_dataset, sampler=tgt_sampler,\n",
    "                                               batch_size=args.batch_size, \n",
    "                                               device=args.device)\n",
    "            \n",
    "            class_running_loss = 0.0\n",
    "            recon_running_loss = 0.0\n",
    "            classifier.train()\n",
    "\n",
    "            for batch_index, (batch_dict, tgt_batch_dict) in enumerate(zip(batch_generator, tgt_batch_generator)):\n",
    "                \n",
    "                if batch_index>500:\n",
    "                    break\n",
    "\n",
    "                # the classifier training routine:\n",
    "                \n",
    "                # step 1. compute the classifier output for source data\n",
    "                y_pred = classifier(batch_dict[\"x_data\"].float())\n",
    "\n",
    "                # step 2. compute the bce loss for classifier output\n",
    "                loss_class = bce_loss_func(y_pred, batch_dict['y_target'].float())\n",
    "                loss_class_w = loss_class*1\n",
    "\n",
    "                \n",
    "                if batch_index%10==0:\n",
    "\n",
    "                    # --------------------------------------\n",
    "                    # zero the gradients\n",
    "                    opt.zero_grad()\n",
    "\n",
    "\n",
    "                    # step 3. use optimizer to take gradient step\n",
    "                    loss_class_w.backward()\n",
    "                    opt.step()\n",
    "\n",
    "                    # -----------------------------------------\n",
    "                    \n",
    "                # compute the loss for update\n",
    "                loss_class_t = loss_class.item()\n",
    "                class_running_loss += (loss_class_t - class_running_loss) / (batch_index + 1)\n",
    "                                \n",
    "                # the reconstructor training routine:\n",
    "                \n",
    "                # --------------------------------------\n",
    "                # zero the gradients\n",
    "                opt.zero_grad()\n",
    "\n",
    "                # step 1. compute the reconstructor output for target data\n",
    "                recon = classifier.reconstruct(tgt_batch_dict[\"x_data\"].float())\n",
    "\n",
    "                # step 2. compute mse loss of reconstruction\n",
    "                loss_recon = mse_loss_func(recon, tgt_batch_dict[\"x_data\"].float())\n",
    "                loss_recon = loss_recon\n",
    "                loss_recon_w = loss_recon*1\n",
    "                \n",
    "                # step 3. use optimizer to take gradient step\n",
    "                loss_recon_w.backward()\n",
    "                opt.step()\n",
    "                \n",
    "                # -----------------------------------------\n",
    "                # compute the loss for update\n",
    "                loss_recon_t = loss_recon.item()\n",
    "                recon_running_loss += (loss_recon_t - recon_running_loss) / (batch_index + 1)\n",
    "                \n",
    "                \n",
    "                # update bar\n",
    "                train_bar.set_postfix(recon_loss=recon_running_loss,\n",
    "                                      class_loss=class_running_loss,\n",
    "                                      epoch=epoch_index)\n",
    "                                 \n",
    "                train_bar.update()\n",
    "            \n",
    "            # to check if training goes all the way!!\n",
    "#            assert batch_index>(nsamples//int(args.batch_size))-2\n",
    "\n",
    "            logging.debug(f\"Reconstruction Loss: {recon_running_loss}, Classification Loss: {class_running_loss}\")\n",
    "            # Iterate over val dataset\n",
    "\n",
    "            # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "            # replacing validation on source by validation on target\n",
    "            tgt_dataset.set_split('valid') \n",
    "            batch_generator = utils.generate_batches(tgt_dataset, sampler=tgt_valid_sampler,\n",
    "                                               batch_size=int(args.batch_size*1e1), \n",
    "                                               device=args.device)\n",
    "            running_loss = 0.0\n",
    "            recon_running_loss = 0.0\n",
    "            tmp_filename = f\"./{TF}_{MODEL_NAME}_tmp.tmp\"\n",
    "            tmp_file = open(tmp_filename, \"wb\")\n",
    "            classifier.eval()\n",
    "\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "                # compute the classifier, reconstruction output\n",
    "                x = batch_dict[\"x_data\"].float()\n",
    "                y_pred = classifier(x) # tgt classify\n",
    "                x_pred = classifier.reconstruct(x) # tgt reconstruct\n",
    "                y_target = batch_dict['y_target'].float()\n",
    "                \n",
    "                # compute the classifier, reconstruction loss\n",
    "                loss = bce_loss_func(y_pred, y_target)\n",
    "                loss_recon = mse_loss_func(x_pred, x)\n",
    "\n",
    "                # compute the runnning losses\n",
    "                loss_t = loss.item()\n",
    "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "                loss_recon_t = loss_recon.item()\n",
    "                recon_running_loss += (loss_recon_t - recon_running_loss) / (batch_index + 1)\n",
    "\n",
    "                # save data for computing aps\n",
    "                for yp, yt in zip(torch.sigmoid(y_pred).cpu().detach().numpy(), y_target.cpu().detach().numpy()):\n",
    "                    tmp_file.write(bytes(f\"{yp},{yt}\\n\", \"utf-8\"))\n",
    "\n",
    "                val_bar.set_postfix(loss=running_loss,\n",
    "                                    recon_loss = recon_running_loss,\n",
    "                                    epoch=epoch_index,\n",
    "                                    early_stop=train_state['early_stopping_step'])\n",
    "                val_bar.update()\n",
    "\n",
    "            train_state['val_loss'].append(running_loss)\n",
    "            \n",
    "            # compute aps from saved file\n",
    "            tmp_file.close()\n",
    "            val_aps = utils.compute_aps_from_file(tmp_filename)\n",
    "            os.remove(tmp_filename)\n",
    "        \n",
    "            train_state['val_aps'].append(val_aps)\n",
    "\n",
    "            train_state = utils.update_train_state(args=args, model=classifier,\n",
    "                                             train_state=train_state)\n",
    "            \n",
    "            logging.debug(f\"Epoch: {epoch_index}, Validation Loss: {running_loss}, Validation Reconstruction Loss: {recon_running_loss}, Validation APS: {val_aps}\")\n",
    "\n",
    "            train_bar.n = 0\n",
    "            val_bar.n = 0\n",
    "            epoch_bar.update()\n",
    "\n",
    "            if train_state['stop_early']:\n",
    "                logging.debug(\"Early stopping criterion fulfilled!\")\n",
    "                break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        logging.warning(\"Exiting loop\")\n",
    "    \n",
    "    return train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b8b39bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "075edeb0f3864dcebf6ac8b201b7a2b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e3006376484cdeac460524b022d7cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/7108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249990ba62094a5aa9f207332dcc1b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=valid:   0%|          | 0/3148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/d/dzb5732/work/.dda/lib/python3.7/site-packages/torch/nn/modules/conv.py:295: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448224956/work/aten/src/ATen/native/Convolution.cpp:660.)\n",
      "  self.padding, self.dilation, self.groups)\n",
      "/storage/home/d/dzb5732/work/.dda/lib/python3.7/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448224956/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_state = train_drcn(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4844a7",
   "metadata": {},
   "source": [
    "# Scratch\n",
    "\n",
    "1. Evaluate reconstruction\n",
    "2. Evalute replication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a702c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = DRCN(args)\n",
    "classifier.load_state_dict(torch.load(args.model_state_file))\n",
    "classifier = classifier.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19d79112",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dataset = datasets.TFDataset.load_dataset_and_vectorizer_from_path(args.target_csv, \n",
    "                                                                 args.target_genome_fasta, \n",
    "                                                                 ohe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c704e58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_generator = utils.generate_batches(target_dataset, sampler=None, shuffle=False, \n",
    "                                   batch_size=64, \n",
    "                                   device=args.device, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e405bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(batch_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1ec550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_recon = classifier.reconstruct(batch[\"x_data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7482358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = batch[\"x_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7e85c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3200, device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 0\n",
    "torch.sum(torch.argmax(x[n], dim=0)==torch.argmax(x_recon[n], dim=0))/500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54b4704a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 3, 1, 2,\n",
       "        3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 1,\n",
       "        2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2,\n",
       "        0, 2, 0, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 0, 1, 2, 1, 1, 2, 2, 0, 2, 2, 2, 1,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        2, 2, 0, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 0, 2, 1, 1, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 2, 0, 0, 2, 0, 0, 2, 0, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0,\n",
       "        2, 2, 2, 3, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 2, 1, 2, 1, 0, 1, 2, 1, 2, 2, 1, 1, 1, 0, 2, 0, 2, 0,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 0, 0, 0, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(x_recon[n], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "995a4104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 1, 1, 0, 3, 1, 0, 2, 1, 3, 1, 1, 2, 1, 2, 1, 1, 3, 3, 1, 2, 2, 3,\n",
       "        2, 3, 0, 2, 3, 2, 3, 1, 1, 1, 3, 3, 2, 2, 1, 1, 1, 0, 2, 3, 3, 2, 3, 3,\n",
       "        3, 1, 1, 2, 2, 1, 1, 1, 1, 0, 1, 0, 1, 3, 2, 0, 1, 1, 3, 2, 3, 0, 0, 2,\n",
       "        0, 1, 0, 2, 1, 0, 1, 0, 2, 1, 1, 2, 2, 3, 1, 0, 1, 3, 1, 2, 0, 1, 2, 2,\n",
       "        1, 1, 0, 2, 2, 3, 0, 3, 0, 1, 2, 2, 3, 1, 0, 3, 1, 0, 2, 3, 2, 2, 3, 1,\n",
       "        0, 1, 1, 0, 1, 1, 0, 3, 0, 0, 3, 2, 1, 0, 2, 0, 0, 0, 2, 0, 2, 1, 1, 0,\n",
       "        0, 2, 1, 2, 3, 1, 0, 1, 0, 1, 2, 3, 2, 0, 2, 2, 3, 2, 0, 2, 0, 2, 1, 0,\n",
       "        1, 1, 2, 3, 3, 1, 2, 1, 1, 1, 3, 2, 1, 0, 2, 2, 3, 2, 2, 0, 2, 1, 0, 2,\n",
       "        0, 3, 2, 0, 0, 0, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 0, 2, 0, 2, 3, 3, 0,\n",
       "        1, 0, 2, 2, 0, 1, 0, 2, 1, 0, 2, 1, 3, 3, 1, 1, 1, 1, 3, 2, 3, 3, 0, 2,\n",
       "        2, 0, 0, 3, 3, 0, 0, 2, 0, 1, 0, 2, 2, 0, 2, 3, 1, 0, 0, 0, 1, 1, 3, 2,\n",
       "        0, 2, 0, 1, 2, 2, 2, 3, 3, 1, 0, 1, 0, 2, 0, 1, 1, 3, 1, 2, 1, 3, 2, 1,\n",
       "        0, 2, 2, 3, 2, 2, 1, 3, 1, 1, 3, 2, 1, 1, 1, 0, 3, 3, 1, 3, 1, 0, 2, 2,\n",
       "        0, 0, 0, 2, 2, 1, 0, 2, 3, 0, 2, 1, 1, 0, 1, 2, 2, 1, 1, 1, 1, 0, 2, 1,\n",
       "        3, 1, 0, 2, 2, 3, 3, 1, 3, 3, 2, 1, 0, 2, 2, 2, 0, 2, 3, 3, 3, 0, 1, 0,\n",
       "        3, 1, 0, 2, 3, 0, 0, 1, 3, 1, 1, 3, 1, 0, 1, 1, 3, 3, 2, 0, 2, 2, 0, 2,\n",
       "        0, 1, 0, 1, 1, 0, 2, 2, 2, 1, 1, 3, 3, 1, 1, 3, 1, 1, 1, 2, 0, 0, 2, 1,\n",
       "        1, 1, 0, 3, 3, 3, 0, 2, 2, 0, 2, 0, 2, 2, 1, 0, 2, 0, 3, 3, 2, 0, 2, 1,\n",
       "        2, 0, 1, 3, 1, 2, 0, 1, 3, 1, 2, 2, 0, 2, 2, 0, 3, 0, 2, 2, 0, 2, 2, 2,\n",
       "        3, 2, 3, 3, 1, 0, 2, 2, 2, 2, 1, 1, 1, 3, 0, 2, 1, 3, 1, 1, 0, 1, 0, 2,\n",
       "        3, 3, 1, 1, 1, 0, 2, 0, 2, 2, 0, 3, 2, 0, 1, 1, 3, 3, 0, 2],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(x[n], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aeda1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
