{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed5498ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "from argparse import Namespace\n",
    "import tqdm\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61afbad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GLOBALS \n",
    "SOURCE_GENOME=\"mm10\"\n",
    "TF=\"RXRA\"\n",
    "SOURCE_GENOME_FASTA='../../genomes/mm10_no_alt_analysis_set_ENCODE.fasta'\n",
    "TARGET_GENOME = \"hg38\"\n",
    "TARGET_GENOME_FASTA = \"../../genomes/GRCh38_no_alt_analysis_set_GCA_000001405.15.fasta\"\n",
    "PILOT_STUDY=False\n",
    "MODEL_NAME=\"drcn\"\n",
    "PYTORCH_DEVICE=\"cuda\"\n",
    "TRAIN=True\n",
    "MODEL_STORAGE_SUFFIX=\"_pilot\" if PILOT_STUDY else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1d48cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../\")\n",
    "from utils import datasets,samplers,models,utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "252f361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logger config\n",
    "logging.basicConfig(filename=f'./log/{TF}_{MODEL_NAME}{MODEL_STORAGE_SUFFIX}.log', filemode='w', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507ef799",
   "metadata": {},
   "source": [
    "# Define namespace arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4d07c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\t../../torch_models/mm10/RXRA/drcn/drcn.pth\n",
      "Using CUDA: True\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path information\n",
    "    model_state_file=f'{MODEL_NAME}{MODEL_STORAGE_SUFFIX}.pth',\n",
    "    source_csv=f'../../data/{SOURCE_GENOME}/{TF}/split_data.csv.gz',\n",
    "    source_genome_fasta=SOURCE_GENOME_FASTA,\n",
    "    target_csv = f'../../data/{TARGET_GENOME}/{TF}/split_data.csv.gz',\n",
    "    target_genome_fasta = TARGET_GENOME_FASTA,\n",
    "    model_save_dir=f'../../torch_models/{SOURCE_GENOME}/{TF}/{MODEL_NAME}/',\n",
    "    results_save_dir=f'../../results/{SOURCE_GENOME}/{TF}/',\n",
    "    feat_size=(4, 500),\n",
    "    \n",
    "    # Model hyper parameters\n",
    "    conv_filters=240,\n",
    "    conv_kernelsize=20,\n",
    "    maxpool_strides=15,\n",
    "    maxpool_size=15,\n",
    "    lstm_outnodes=32,\n",
    "    linear1_nodes=1024,\n",
    "    dropout_prob=0.5,\n",
    "    \n",
    "    # Training hyper parameters\n",
    "    batch_size=128,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.0001,\n",
    "    num_epochs=20,\n",
    "    tolerance=0,\n",
    "    seed=7,\n",
    "    \n",
    "    # Runtime options\n",
    "    catch_keyboard_interrupt=True,\n",
    "    cuda=True if PYTORCH_DEVICE==\"cuda\" else False,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    pilot=PILOT_STUDY, # 2% of original dataset\n",
    "    train=TRAIN,\n",
    "    test_batch_size=int(2e3)\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "\n",
    "    args.model_state_file = os.path.join(args.model_save_dir,\n",
    "                                         args.model_state_file)\n",
    "    \n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "    \n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "utils.set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "utils.handle_dirs(args.model_save_dir)\n",
    "utils.handle_dirs(args.results_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5db264",
   "metadata": {},
   "source": [
    "# DRCN Classifier-Reconstructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62a6bcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Conventional model with reconstruction\n",
    "\n",
    "class encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        super(encoder, self).__init__()\n",
    "        self.featurizer=models.TFCNN(channels=args.feat_size[0], \n",
    "                             conv_filters=args.conv_filters, \n",
    "                             conv_kernelsize=args.conv_kernelsize, \n",
    "                             maxpool_size=args.maxpool_size, \n",
    "                             maxpool_strides=args.maxpool_strides)\n",
    "        \n",
    "    def forward(self, x_in):\n",
    "        x_in = self.featurizer(x_in)\n",
    "        return x_in\n",
    "    \n",
    "class decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        super(decoder, self).__init__()\n",
    "        self.dconv0 = nn.ConvTranspose1d(args.conv_filters, 120, kernel_size=4, stride=1)\n",
    "        self.bn0 = nn.BatchNorm1d(120, affine=False)\n",
    "        self.prelu0 = nn.PReLU()\n",
    "        self.dconv1 = nn.ConvTranspose1d(120, 64, kernel_size=3, stride=2, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(64, affine=False)\n",
    "        self.prelu1 = nn.PReLU()\n",
    "        self.dconv2 = nn.ConvTranspose1d(64,32, kernel_size=3, stride=2, padding=4)\n",
    "        self.bn2 = nn.BatchNorm1d(32, affine=False)\n",
    "        self.prelu2 = nn.PReLU()\n",
    "        self.dconv3 = nn.ConvTranspose1d(32, 16, kernel_size=3, stride=2, padding=4)\n",
    "        self.bn3 = nn.BatchNorm1d(16, affine=False)\n",
    "        self.prelu3 = nn.PReLU()\n",
    "        self.dconv4 = nn.ConvTranspose1d(16, 8, kernel_size=3, stride=2, padding=4)\n",
    "        self.bn4 = nn.BatchNorm1d(8, affine=False)\n",
    "        self.prelu4 = nn.PReLU()\n",
    "        self.dconv5 = nn.ConvTranspose1d(8, 4, kernel_size=2, stride=1, padding=2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.prelu0(self.bn0(self.dconv0(x)))\n",
    "        x = self.prelu1(self.bn1(self.dconv1(x)))\n",
    "        x = self.prelu2(self.bn2(self.dconv2(x)))\n",
    "        x = self.prelu3(self.bn3(self.dconv3(x)))\n",
    "        x = self.prelu4(self.bn4(self.dconv4(x)))\n",
    "        x = self.softmax(self.dconv5(x))\n",
    "        return x\n",
    "\n",
    "class DRCN(nn.Module):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        super(DRCN, self).__init__()\n",
    "        self.featurizer=encoder(args)\n",
    "        self.classifier=models.TFLSTM(input_features=args.conv_filters, lstm_nodes=args.lstm_outnodes, \n",
    "                               fc1_nodes=args.linear1_nodes)\n",
    "        \n",
    "        self.decoder=decoder(args)\n",
    "        \n",
    "    def forward(self, x_in, apply_sigmoid=False):\n",
    "        x_in = self.featurizer(x_in)\n",
    "        class_out = self.classifier(x_in, apply_sigmoid=apply_sigmoid)\n",
    "        return class_out\n",
    "    \n",
    "    def reconstruct(self, x):\n",
    "        x = self.featurizer(x)\n",
    "        recon_x = self.decoder(x)\n",
    "        return recon_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bff15f",
   "metadata": {},
   "source": [
    "# DRCN Training Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da91f086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_drcn_from_hybrid(drcn, hybrid_path):\n",
    "    hybrid_state_dict = torch.load(hybrid_path)\n",
    "    \n",
    "    pretrained_dict = {}\n",
    "\n",
    "    for k,v in hybrid_state_dict.items():\n",
    "        if k.startswith(\"featurizer\"):\n",
    "            newk = \"featurizer.\" + k\n",
    "            pretrained_dict[newk] = v\n",
    "        else:\n",
    "            pretrained_dict[k] = v\n",
    "    \n",
    "    drcn.load_state_dict(pretrained_dict, strict=False)\n",
    "    \n",
    "    return drcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96df60a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_requires_grad(model, requires_grad=True):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad=requires_grad\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8439a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_drcn(args):\n",
    "    \n",
    "    # Load the dataset\n",
    "    logging.debug(f'Loading source and target data...')\n",
    "    src_dataset, tgt_dataset = datasets.load_data(args)\n",
    "    \n",
    "    # Initializing models\n",
    "    logging.debug(f'Initializing model...')\n",
    "    classifier = DRCN(args)\n",
    "    logging.debug(classifier)\n",
    "    hybrid_path = args.model_state_file.replace(MODEL_NAME, \"hybrid\")\n",
    "    \n",
    "    # load from hybrid path\n",
    "#     if os.path.exists(hybrid_path):\n",
    "#         classifier = load_drcn_from_hybrid(classifier, hybrid_path)\n",
    "#     else:\n",
    "#         logging.error(\"Hybrid model path not found. Results will not be as expected!\")\n",
    "\n",
    "    # load model from path if available\n",
    "#     if os.path.exists(args.model_state_file):\n",
    "#         classifier.load_state_dict(torch.load(args.model_state_file))\n",
    "#         logging.info(\"Previously trained model loaded!\")\n",
    "        \n",
    "    \n",
    "        \n",
    "    classifier.to(args.device)\n",
    "    model_params = utils.get_n_params(classifier)\n",
    "    logging.debug(f\"The model has {model_params} parameters.\")\n",
    "        \n",
    "    # Defining loss functions, optimizers\n",
    "    bce_loss_func = nn.BCEWithLogitsLoss()\n",
    "    mse_loss_func = nn.MSELoss()\n",
    "    opt = optim.Adam(classifier.parameters(), lr=args.learning_rate, eps=1e-7)\n",
    "    \n",
    "    logging.debug(\"Making samplers...\")\n",
    "    # weighted train samplers for classifier part of the model\n",
    "    src_dataset.set_split(\"train\")\n",
    "    train_sampler = samplers.get_sampler(src_dataset, weighted=True, mini=False)\n",
    "    nsamples = train_sampler.num_samples\n",
    "    \n",
    "    # unweighted sampler of target data for reconstruction loss \n",
    "    tgt_dataset.set_split(\"train\")\n",
    "    tgt_sampler = samplers.get_sampler(tgt_dataset, weighted=False, mini=False)\n",
    "    \n",
    "    # tqdm progress bar initialize\n",
    "    epoch_bar = tqdm.notebook.tqdm(desc='training routine', \n",
    "                          total=args.num_epochs,\n",
    "                          position=0)\n",
    "    \n",
    "    train_bar = tqdm.notebook.tqdm(desc=f'split=train',\n",
    "                              total=nsamples//int(args.batch_size), \n",
    "                              position=1, \n",
    "                              leave=True)\n",
    "    \n",
    "    \n",
    "    ##### Training Routine #####\n",
    "    \n",
    "    try:\n",
    "        for epoch_index in range(args.num_epochs):\n",
    "\n",
    "            # Iterate over training dataset\n",
    "\n",
    "            # setup: batch generator (w), tgt_batch_generator (uw)\n",
    "            # set loss and acc to 0, \n",
    "            # set train mode on\n",
    "            src_dataset.set_split('train')\n",
    "            batch_generator = utils.generate_batches(src_dataset, sampler=train_sampler,\n",
    "                                               batch_size=args.batch_size, \n",
    "                                               device=args.device)\n",
    "\n",
    "            tgt_dataset.set_split('train')\n",
    "            tgt_batch_generator = utils.generate_batches(tgt_dataset, sampler=tgt_sampler,\n",
    "                                               batch_size=args.batch_size, \n",
    "                                               device=args.device)\n",
    "            \n",
    "            class_running_loss = 0.0\n",
    "            recon_running_loss = 0.0\n",
    "            classifier.train()\n",
    "\n",
    "            for batch_index, (batch_dict, tgt_batch_dict) in enumerate(zip(batch_generator, tgt_batch_generator)):\n",
    "\n",
    "                # early stopping if loaded from hybrid\n",
    "#                 if batch_index>500:\n",
    "#                     break\n",
    "\n",
    "                # the classifier training routine:\n",
    "                \n",
    "                # step 1. compute the classifier output for source data\n",
    "                y_pred = classifier(batch_dict[\"x_data\"].float())\n",
    "\n",
    "                # step 2. compute the bce loss for classifier output\n",
    "                loss_class = bce_loss_func(y_pred, batch_dict['y_target'].float())\n",
    "                loss_class_w = loss_class*1\n",
    "                \n",
    "                # train classifier per 10 batches\n",
    "                if batch_index%10==0:\n",
    "\n",
    "                    # --------------------------------------\n",
    "                    # zero the gradients\n",
    "                    opt.zero_grad()\n",
    "\n",
    "                    # step 3. use optimizer to take gradient step\n",
    "                    loss_class_w.backward()\n",
    "                    opt.step()\n",
    "                \n",
    "                # -----------------------------------------\n",
    "                # compute the loss for update\n",
    "                loss_class_t = loss_class.item()\n",
    "                class_running_loss += (loss_class_t - class_running_loss) / (batch_index + 1)\n",
    "                                \n",
    "                # the reconstructor training routine:\n",
    "                \n",
    "                # --------------------------------------\n",
    "                # zero the gradients\n",
    "                opt.zero_grad()\n",
    "\n",
    "                # step 1. compute the reconstructor output for target data\n",
    "                recon = classifier.reconstruct(tgt_batch_dict[\"x_data\"].float())\n",
    "\n",
    "                # step 2. compute mse loss of reconstruction\n",
    "                loss_recon = mse_loss_func(recon, tgt_batch_dict[\"x_data\"].float())\n",
    "                loss_recon = loss_recon\n",
    "                loss_recon_w = loss_recon*1\n",
    "                \n",
    "                # step 3. use optimizer to take gradient step\n",
    "                loss_recon_w.backward()\n",
    "                opt.step()\n",
    "                \n",
    "                # -----------------------------------------\n",
    "                # compute the loss for update\n",
    "                loss_recon_t = loss_recon.item()\n",
    "                recon_running_loss += (loss_recon_t - recon_running_loss) / (batch_index + 1)\n",
    "                \n",
    "                \n",
    "                # update bar\n",
    "                train_bar.set_postfix(recon_loss=recon_running_loss,\n",
    "                                      class_loss=class_running_loss,\n",
    "                                      epoch=epoch_index)\n",
    "                                 \n",
    "                train_bar.update()\n",
    "            \n",
    "            torch.save(classifier.state_dict(), args.model_state_file)\n",
    "\n",
    "            logging.debug(f\"Epoch: {epoch_index}, Reconstruction Loss: {recon_running_loss}, Classification Loss: {class_running_loss}\")\n",
    "            logging.debug(f\"Model saved at {args.model_state_file}\")\n",
    "\n",
    "            train_bar.n = 0\n",
    "            epoch_bar.update()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        logging.warning(\"Exiting loop\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b8b39bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dd5e43e8ca343a0a7e99ae87cb00ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "259e412846df44b38b0f080635d81dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/7524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/d/dzb5732/work/.dda/lib/python3.7/site-packages/torch/nn/modules/conv.py:295: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448224956/work/aten/src/ATen/native/Convolution.cpp:660.)\n",
      "  self.padding, self.dilation, self.groups)\n",
      "/storage/home/d/dzb5732/work/.dda/lib/python3.7/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448224956/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_drcn(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b947f114",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd7410b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = DRCN(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69339f60",
   "metadata": {},
   "source": [
    "## Target dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f35434da",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dataset = datasets.TFDataset.load_dataset_and_vectorizer_from_path(args.target_csv, \n",
    "                                                                 args.target_genome_fasta, \n",
    "                                                                 ohe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cfd81f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "split=test:   0%|          | 3/2169 [00:03<31:08,  1.16it/s, aps=0.302, batch=2, loss=0.528]  /storage/home/d/dzb5732/work/.dda/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:681: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "split=test: 100%|██████████| 2169/2169 [14:28<00:00,  3.93it/s, aps=nan, batch=2169, loss=0.491]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../../results/mm10/RXRA/drcn_tgt.csv.gz'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.eval_model(classifier, target_dataset, args, dataset_type=\"tgt\", model=\"drcn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca53889",
   "metadata": {},
   "source": [
    "## Source dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c94a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dataset = datasets.TFDataset.load_dataset_and_vectorizer_from_path(args.source_csv, \n",
    "                                                                          args.source_genome_fasta, \n",
    "                                                                          ohe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ccb606",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "utils.eval_model(classifier, source_dataset, args, dataset_type=\"src\", model=\"drcn\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
