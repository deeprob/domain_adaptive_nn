{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed5498ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "from argparse import Namespace\n",
    "import tqdm\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61afbad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GLOBALS \n",
    "SOURCE_GENOME=\"mm10\"\n",
    "TF=\"CEBPA\"\n",
    "SOURCE_GENOME_FASTA='../../genomes/mm10_no_alt_analysis_set_ENCODE.fasta'\n",
    "TARGET_GENOME = \"hg38\"\n",
    "TARGET_GENOME_FASTA = \"../../genomes/GRCh38_no_alt_analysis_set_GCA_000001405.15.fasta\"\n",
    "PILOT_STUDY=False\n",
    "MODEL_NAME=\"cogan\"\n",
    "PYTORCH_DEVICE=\"cuda\"\n",
    "TRAIN=True\n",
    "MODEL_STORAGE_SUFFIX=\"_pilot\" if PILOT_STUDY else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1d48cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../\")\n",
    "from utils import datasets,samplers,models,utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "252f361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logger config\n",
    "logging.basicConfig(filename=f'./log/{TF}_{MODEL_NAME}{MODEL_STORAGE_SUFFIX}.log', filemode='w', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507ef799",
   "metadata": {},
   "source": [
    "# Define namespace arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4d07c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\t../torch_models/mm10/CEBPA/cogan/cogan.pth\n",
      "Using CUDA: False\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path information\n",
    "    model_state_file=f'{MODEL_NAME}{MODEL_STORAGE_SUFFIX}.pth',\n",
    "    source_csv=f'../data/{SOURCE_GENOME}/{TF}/split_data.csv.gz',\n",
    "    source_genome_fasta=SOURCE_GENOME_FASTA,\n",
    "    target_csv = f'../data/{TARGET_GENOME}/{TF}/split_data.csv.gz',\n",
    "    target_genome_fasta = TARGET_GENOME_FASTA,\n",
    "    model_save_dir=f'../torch_models/{SOURCE_GENOME}/{TF}/{MODEL_NAME}/',\n",
    "    results_save_dir=f'../results/{SOURCE_GENOME}/{TF}/',\n",
    "    feat_size=(4, 500),\n",
    "    \n",
    "    # Model hyper parameters\n",
    "    conv_filters=240,\n",
    "    conv_kernelsize=20,\n",
    "    maxpool_strides=15,\n",
    "    maxpool_size=15,\n",
    "    lstm_outnodes=32,\n",
    "    linear1_nodes=1024,\n",
    "    dropout_prob=0.5,\n",
    "    \n",
    "    # Training hyper parameters\n",
    "    batch_size=128,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=15,\n",
    "    tolerance=1e-3,\n",
    "    seed=1337,\n",
    "    \n",
    "    # Runtime options\n",
    "    catch_keyboard_interrupt=True,\n",
    "    cuda=True if PYTORCH_DEVICE==\"cuda\" else False,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    pilot=PILOT_STUDY, # 2% of original dataset\n",
    "    train=TRAIN,\n",
    "    test_batch_size=int(2e3)\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "\n",
    "    args.model_state_file = os.path.join(args.model_save_dir,\n",
    "                                         args.model_state_file)\n",
    "    \n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "    \n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "utils.set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "utils.handle_dirs(args.model_save_dir)\n",
    "utils.handle_dirs(args.results_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5db264",
   "metadata": {},
   "source": [
    "# CoGAN CNN-RNN Classifier/Discriminator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5fc1ac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFCoCD(nn.Module):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        super(TFCoCD, self).__init__()\n",
    "        self.featurizer=models.TFCNN(channels=args.feat_size[0], \n",
    "                                     conv_filters=args.conv_filters, \n",
    "                                     conv_kernelsize=args.conv_kernelsize, \n",
    "                                     maxpool_size=args.maxpool_size, \n",
    "                                     maxpool_strides=args.maxpool_strides)\n",
    "        self.cd_lstm=models.TFLSTM_(input_features=args.conv_filters, \n",
    "                                    lstm_nodes=args.lstm_outnodes)\n",
    "        self.cd_mlp=models.TFMLP_(input_features=args.lstm_outnodes, \n",
    "                                  fc1_nodes=args.linear1_nodes)\n",
    "        \n",
    "        self.c_slp=models.TFSLP(input_features=args.linear1_nodes//4)\n",
    "        self.d_slp=models.TFSLP(input_features=args.linear1_nodes//4)\n",
    "\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x_a, x_b, apply_sigmoid=False):\n",
    "        x_a = self.featurizer(x_a)\n",
    "        x_a = self.cd_lstm(x_a)\n",
    "        x_b = self.featurizer(x_b)\n",
    "        x_b = self.cd_lstm(x_b)\n",
    "        x_in = torch.cat((x_a, x_b))\n",
    "        x_in = self.cd_mlp(x_in)\n",
    "        \n",
    "        out_dscm = self.d_slp(x_in)\n",
    "        if apply_sigmoid:\n",
    "            out_dscm = torch.sigmoid(out_dscm)\n",
    "\n",
    "        return out_dscm, x_a, x_b\n",
    "    \n",
    "    def classify_a(self, x_a):\n",
    "        x_a = self.featurizer(x_a)\n",
    "        x_a = self.cd_lstm(x_a)\n",
    "        x_in = self.cd_mlp(x_a)\n",
    "        \n",
    "        out_class = self.c_slp(x_in)\n",
    "        if apply_sigmoid:\n",
    "            out_class = torch.sigmoid(out_class)\n",
    "\n",
    "        return out_class\n",
    "\n",
    "    def classify_b(self, x_b):\n",
    "        x_b = self.featurizer(x_b)\n",
    "        x_b = self.cd_lstm(x_b)\n",
    "        x_in = self.cd_mlp(x_b)\n",
    "        \n",
    "        out_class = self.c_slp(x_in)\n",
    "        if apply_sigmoid:\n",
    "            out_class = torch.sigmoid(out_class)\n",
    "\n",
    "        return out_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3a02bcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFCoGen(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dims):\n",
    "        super(TFCoGen, self).__init__()\n",
    "        self.dconv0 = nn.ConvTranspose1d(latent_dims, 1024, kernel_size=15, stride=1)\n",
    "        self.bn0 = nn.BatchNorm1d(1024, affine=False)\n",
    "        self.prelu0 = nn.PReLU()\n",
    "        self.dconv1 = nn.ConvTranspose1d(1024, 512, kernel_size=6, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(512, affine=False)\n",
    "        self.prelu1 = nn.PReLU()\n",
    "        self.dconv2 = nn.ConvTranspose1d(512, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(256, affine=False)\n",
    "        self.prelu2 = nn.PReLU()\n",
    "        self.dconv3 = nn.ConvTranspose1d(256, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(128, affine=False)\n",
    "        self.prelu3 = nn.PReLU()\n",
    "        self.dconv4 = nn.ConvTranspose1d(128, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm1d(64, affine=False)\n",
    "        self.prelu4 = nn.PReLU()\n",
    "        self.dconv5 = nn.ConvTranspose1d(64, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn5 = nn.BatchNorm1d(32, affine=False)\n",
    "        self.prelu5 = nn.PReLU()\n",
    "        self.dconv6_a = nn.ConvTranspose1d(32, 4, kernel_size=6, stride=1, padding=1)\n",
    "        self.dconv6_b = nn.ConvTranspose1d(32, 4, kernel_size=6, stride=1, padding=1)\n",
    "        self.sig6_a = nn.Sigmoid()\n",
    "        self.sig6_b = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, z):\n",
    "        z = z.view(z.size(0), z.size(1), 1)\n",
    "        h0 = self.prelu0(self.bn0(self.dconv0(z)))\n",
    "        h1 = self.prelu1(self.bn1(self.dconv1(h0)))\n",
    "        h2 = self.prelu2(self.bn2(self.dconv2(h1)))\n",
    "        h3 = self.prelu3(self.bn3(self.dconv3(h2)))\n",
    "        h4 = self.prelu4(self.bn4(self.dconv4(h3)))\n",
    "        h5 = self.prelu5(self.bn5(self.dconv5(h4)))\n",
    "        out_a = self.sig6_a(self.dconv6_a(h5))\n",
    "        out_b = self.sig6_b(self.dconv6_b(h5))\n",
    "        return out_a, out_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "96df60a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_requires_grad(model, requires_grad=True):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad=requires_grad\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8439a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cogan(args):\n",
    "    \n",
    "    # Load the dataset\n",
    "    logging.debug(f'Loading source and target data...')\n",
    "    src_dataset, tgt_dataset = datasets.load_data(args)\n",
    "    \n",
    "    # Initializing models\n",
    "    logging.debug(f'Initializing model...')\n",
    "    discriminator = TFCoCD(args)\n",
    "    generator = TFCoGen(100)\n",
    "    discriminator.to(args.device)\n",
    "    generator.to(args.device)\n",
    "    model_params = utils.get_n_params(discriminator) + utils.get_n_params(generator)\n",
    "    logging.debug(f\"The model has {model_params} parameters.\")\n",
    "        \n",
    "    # Defining loss functions, optimizers\n",
    "    bce_loss_func = nn.BCEWithLogitsLoss()\n",
    "    mse_loss_func = nn.MSELoss()\n",
    "    opt_dscm = optim.Adam(discriminator.parameters(), lr=0.0002, eps=1e-7, weight_decay=0.0005)\n",
    "    opt_gen = optim.Adam(generator.parameters(), lr=0.0002, eps=1e-7, weight_decay=0.0005)\n",
    "\n",
    "    \n",
    "    # Making samplers\n",
    "    # weighted train and unweighted valid samplers for classifier part of the model\n",
    "    train_sampler, valid_sampler = samplers.make_train_samplers(src_dataset, args)\n",
    "    nsamples = train_sampler.num_samples\n",
    "    \n",
    "    # unweighted samplers of source and target data for generator and discriminator \n",
    "    src_dataset.set_split(\"train\")\n",
    "    src_sampler = samplers.get_sampler(src_dataset, weighted=False, mini=False)\n",
    "    tgt_dataset.set_split(\"train\")\n",
    "    tgt_sampler = samplers.get_sampler(tgt_dataset, weighted=False, mini=False)\n",
    "    \n",
    "    # Defining initial train state\n",
    "    train_state = utils.make_train_state(args)\n",
    "    \n",
    "    # tqdm progress bar initialize\n",
    "    epoch_bar = tqdm.notebook.tqdm(desc='training routine', \n",
    "                          total=args.num_epochs,\n",
    "                          position=0)\n",
    "    \n",
    "    train_bar = tqdm.notebook.tqdm(desc=f'split=train',\n",
    "                              total=nsamples//args.batch_size, \n",
    "                              position=1, \n",
    "                              leave=True)\n",
    "    \n",
    "    src_dataset.set_split('valid')\n",
    "    val_bar = tqdm.notebook.tqdm(desc='split=valid',\n",
    "                        total=len(src_dataset)//int(args.batch_size*1e1), \n",
    "                        position=2, \n",
    "                        leave=True)\n",
    "    \n",
    "    ##### Training Routine #####\n",
    "    \n",
    "    try:\n",
    "        for epoch_index in range(args.num_epochs):\n",
    "            train_state['epoch_index'] = epoch_index\n",
    "\n",
    "            # Iterate over training dataset\n",
    "\n",
    "            # setup: batch generator (uw), src_batch_generator(w), tgt_batch_generator (w)\n",
    "            # set loss and acc to 0, \n",
    "            # set train mode on\n",
    "            src_dataset.set_split('train')\n",
    "            batch_generator = utils.generate_batches(src_dataset, sampler=train_sampler,\n",
    "                                               batch_size=int(2*args.batch_size), \n",
    "                                               device=args.device)\n",
    "            src_dataset.set_split('train')\n",
    "            src_batch_generator = utils.generate_batches(src_dataset, sampler=src_sampler,\n",
    "                                               batch_size=args.batch_size, \n",
    "                                               device=args.device)\n",
    "            tgt_dataset.set_split('train')\n",
    "            tgt_batch_generator = utils.generate_batches(src_dataset, sampler=tgt_sampler,\n",
    "                                               batch_size=args.batch_size, \n",
    "                                               device=args.device)\n",
    "            \n",
    "            dscm_running_loss = 0.0\n",
    "            gen_running_loss = 0.0\n",
    "            running_dscmacc = 0.0\n",
    "            discriminator.train()\n",
    "            generator.train()\n",
    "\n",
    "            for batch_index, (batch_dict, src_batch_dict, tgt_batch_dict) in enumerate(zip(batch_generator, src_batch_generator, tgt_batch_generator)):\n",
    "\n",
    "                # the discriminator training routine:\n",
    "                \n",
    "                # only discriminator gets trained in this step\n",
    "                set_requires_grad(discriminator, requires_grad=True)\n",
    "                set_requires_grad(generator, requires_grad=False)\n",
    "\n",
    "                # --------------------------------------\n",
    "                # zero the gradients\n",
    "                opt_dscm.zero_grad()\n",
    "\n",
    "                # step 1. compute the discriminator output of the discriminator for real data\n",
    "                real_pred, real_feat_a, real_feat_b = discriminator(src_batch_dict[\"x_data\"].float(), tgt_batch_dict[\"x_data\"].float())\n",
    "                real_labels = torch.ones(2*args.batch_size, dtype=torch.float, device=args.device)\n",
    "                dscm_real_loss = bce_loss_func(real_pred, real_labels)\n",
    "\n",
    "                # step 2. compute the discriminator output of the discriminator for fake data\n",
    "                noise = torch.randn((args.batch_size, 100), device=args.device)\n",
    "                fake_data_a, fake_data_b = generator(noise)\n",
    "                fake_pred, fake_feat_a, fake_feat_b = discriminator(fake_data_a, fake_data_b)\n",
    "                fake_labels = torch.zeros(2*args.batch_size, dtype=torch.float, device=args.device)\n",
    "                dscm_fake_loss = bce_loss_func(fake_pred, fake_labels)\n",
    "                dummy_tensor = torch.zeros((fake_feat_a.size(0), fake_feat_a.size(1), fake_feat_a.size(2)), dtype=torch.float, device=args.device)\n",
    "                mse_loss = mse_loss_func(fake_feat_a - fake_feat_b, dummy_tensor)*fake_feat_a.size(1)*fake_feat_a.size(2)\n",
    "                \n",
    "                # step 3. compute the classifier output of the discriminator for real weighted data\n",
    "                class_pred = discriminator.classify_a(batch_dict[\"x_data\"].float())\n",
    "                loss_class = bce_loss_func(class_pred, batch_dict['y_target'].float())\n",
    "                \n",
    "                # Step 4: compute overall loss\n",
    "                loss = dscm_real_loss + dscm_fake_loss + 0.01*mse_loss + 10*loss_class\n",
    "\n",
    "                # step 5. use optimizer to take gradient step\n",
    "                loss.backward()\n",
    "                opt_dscm.step()\n",
    "                \n",
    "                # -----------------------------------------\n",
    "                # compute the loss for update\n",
    "                loss_t = loss.item()\n",
    "                dscm_running_loss += (loss_t - dscm_running_loss) / (batch_index + 1)\n",
    "                \n",
    "                real_hat = real_pred>0.5\n",
    "                real_hat = real_hat.long()\n",
    "                fake_hat = fake_pred<0.5\n",
    "                fake_hat = fake_hat.long()\n",
    "                acc = torch.sum(torch.cat((real_hat, fake_hat)))/(len(real_hat) + len(fake_hat))\n",
    "                acc = acc.item()\n",
    "                running_dscmacc += (acc - running_dscmacc) / (batch_index + 1)\n",
    "                \n",
    "                \n",
    "                # the generator training routine:\n",
    "                \n",
    "                # only generator gets trained in this step\n",
    "                set_requires_grad(discriminator, requires_grad=False)\n",
    "                set_requires_grad(generator, requires_grad=True)\n",
    "                \n",
    "                \n",
    "                # --------------------------------------\n",
    "                # zero the gradients\n",
    "                opt_gen.zero_grad()\n",
    "\n",
    "                # step 1. compute the discriminator output of the discriminator for fake data\n",
    "                noise = torch.randn((args.batch_size, 100), device=args.device)\n",
    "                fake_data_a, fake_data_b = generator(noise)\n",
    "                fake_pred, fake_feat_a, fake_feat_b = discriminator(fake_data_a, fake_data_b)\n",
    "                fake_labels = torch.ones(2*args.batch_size, dtype=torch.float, device=args.device)\n",
    "                gen_fake_loss = bce_loss_func(fake_pred, fake_labels)\n",
    "\n",
    "                # step 2. use optimizer to take gradient step\n",
    "                gen_fake_loss.backward()\n",
    "                opt_gen.step()\n",
    "                \n",
    "                # -----------------------------------------\n",
    "                # compute the loss for update\n",
    "                loss_t = gen_fake_loss.item()\n",
    "                gen_running_loss += (loss_t - gen_running_loss) / (batch_index + 1)\n",
    "                \n",
    "                \n",
    "                # update bar\n",
    "                train_bar.set_postfix(dscm_loss=dscm_running_loss,\n",
    "                                      gen_loss=gen_running_loss,\n",
    "                                      acc=running_dscmacc,\n",
    "                                      epoch=epoch_index)\n",
    "                                 \n",
    "                train_bar.update()\n",
    "\n",
    "\n",
    "            # Iterate over val dataset\n",
    "\n",
    "            # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "            src_dataset.set_split('valid')\n",
    "            batch_generator = utils.generate_batches(src_dataset, sampler=valid_sampler,\n",
    "                                               batch_size=int(args.batch_size*1e1), \n",
    "                                               device=args.device)\n",
    "            running_loss = 0.\n",
    "            ## TODO::Calculate actual aps\n",
    "            tmp_filename = f\"./{TF}_cogan_tmp.tmp\"\n",
    "            tmp_file = open(tmp_filename, \"wb\")\n",
    "            discriminator.eval()\n",
    "            generator.eval()\n",
    "\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "                # compute the output\n",
    "                y_pred = discriminator.classify_a(batch_dict[\"x_data\"].float())\n",
    "                y_target = batch_dict['y_target'].float()\n",
    "                loss = bce_loss_func(class_pred, y_target)\n",
    "\n",
    "                # step 3. compute the loss\n",
    "                loss_t = loss.item()\n",
    "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "                # save data for computing aps\n",
    "                for yp, yt in zip(torch.sigmoid(y_pred).cpu().detach().numpy(), y_target.cpu().detach().numpy()):\n",
    "                    tmp_file.write(bytes(f\"{yp},{yt}\\n\", \"utf-8\"))\n",
    "\n",
    "                val_bar.set_postfix(loss=running_loss,\n",
    "                                    epoch=epoch_index,\n",
    "                                    early_stop=train_state['early_stopping_step'])\n",
    "                val_bar.update()\n",
    "\n",
    "            train_state['val_loss'].append(running_loss)\n",
    "            \n",
    "            # compute aps from saved file\n",
    "            tmp_file.close()\n",
    "            val_aps = utils.compute_aps_from_file(tmp_filename)\n",
    "            os.remove(tmp_filename)\n",
    "        \n",
    "            train_state['val_aps'].append(val_aps)\n",
    "\n",
    "            train_state = utils.update_train_state(args=args, model=classifier,\n",
    "                                             train_state=train_state)\n",
    "\n",
    "            \n",
    "            logging.debug(f\"Epoch: {epoch_index}, Validation Loss: {running_loss}, Validation APS: {val_aps}\")\n",
    "\n",
    "            train_bar.n = 0\n",
    "            val_bar.n = 0\n",
    "            epoch_bar.update()\n",
    "\n",
    "            if train_state['stop_early']:\n",
    "                logging.debug(\"Early stopping criterion fulfilled!\")\n",
    "                break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        logging.warning(\"Exiting loop\")\n",
    "    \n",
    "    return train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b8b39bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a66c6e852cd447f1a33273757aaf2f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ba76afe69374556804d03b7ff0a5832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/9644 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e26bf6103e4c32b3bf2242f5ba66b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=valid:   0%|          | 0/2645 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/d/dzb5732/work/.dda/lib/python3.7/site-packages/torch/nn/modules/conv.py:295: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448224956/work/aten/src/ATen/native/Convolution.cpp:660.)\n",
      "  self.padding, self.dilation, self.groups)\n",
      "/storage/home/d/dzb5732/work/.dda/lib/python3.7/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448224956/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_state = train_cogan(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b947f114",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca53889",
   "metadata": {},
   "source": [
    "## Source dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd7410b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = TFHybrid(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4c94a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dataset = datasets.TFDataset.load_dataset_and_vectorizer_from_path(args.source_csv, \n",
    "                                                                          args.source_genome_fasta, \n",
    "                                                                          ohe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5ccb606",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12fa1c6b6b0425daf0eac151f64f9d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=test:   0%|          | 0/1582 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/d/dzb5732/work/.dda/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:681: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../results/mm10/CEBPA/hybrid_src.csv.gz'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.eval_model(classifier, source_dataset, args, dataset_type=\"src\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69339f60",
   "metadata": {},
   "source": [
    "## Target dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f35434da",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dataset = datasets.TFDataset.load_dataset_and_vectorizer_from_path(args.target_csv, \n",
    "                                                                 args.target_genome_fasta, \n",
    "                                                                 ohe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cfd81f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d52681a04e6467291cb16c1fdfd0659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=test:   0%|          | 0/2169 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'../results/mm10/CEBPA/hybrid_tgt.csv.gz'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.eval_model(classifier, target_dataset, args, dataset_type=\"tgt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd2dcae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
