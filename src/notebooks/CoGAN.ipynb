{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed5498ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "from argparse import Namespace\n",
    "import tqdm\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61afbad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GLOBALS \n",
    "SOURCE_GENOME=\"mm10\"\n",
    "TF=\"CEBPA\"\n",
    "SOURCE_GENOME_FASTA='../../genomes/mm10_no_alt_analysis_set_ENCODE.fasta'\n",
    "TARGET_GENOME = \"hg38\"\n",
    "TARGET_GENOME_FASTA = \"../../genomes/GRCh38_no_alt_analysis_set_GCA_000001405.15.fasta\"\n",
    "PILOT_STUDY=False\n",
    "MODEL_NAME=\"hybrid\"\n",
    "PYTORCH_DEVICE=\"cuda\"\n",
    "TRAIN=True\n",
    "MODEL_STORAGE_SUFFIX=\"_pilot\" if PILOT_STUDY else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1d48cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../\")\n",
    "from utils import datasets,samplers,models,utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "252f361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logger config\n",
    "logging.basicConfig(filename=f'./log/{TF}_{MODEL_NAME}{MODEL_STORAGE_SUFFIX}.log', filemode='w', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507ef799",
   "metadata": {},
   "source": [
    "# Define namespace arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4d07c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\t../torch_models/mm10/CEBPA/hybrid/hybrid.pth\n",
      "Using CUDA: True\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path information\n",
    "    model_state_file=f'{MODEL_NAME}{MODEL_STORAGE_SUFFIX}.pth',\n",
    "    source_csv=f'../data/{SOURCE_GENOME}/{TF}/split_data.csv.gz',\n",
    "    source_genome_fasta=SOURCE_GENOME_FASTA,\n",
    "    target_csv = f'../data/{TARGET_GENOME}/{TF}/split_data.csv.gz',\n",
    "    target_genome_fasta = TARGET_GENOME_FASTA,\n",
    "    model_save_dir=f'../torch_models/{SOURCE_GENOME}/{TF}/{MODEL_NAME}/',\n",
    "    results_save_dir=f'../results/{SOURCE_GENOME}/{TF}/',\n",
    "    feat_size=(4, 500),\n",
    "    \n",
    "    # Model hyper parameters\n",
    "    conv_filters=240,\n",
    "    conv_kernelsize=20,\n",
    "    maxpool_strides=15,\n",
    "    maxpool_size=15,\n",
    "    lstm_outnodes=32,\n",
    "    linear1_nodes=1024,\n",
    "    dropout_prob=0.5,\n",
    "    \n",
    "    # Training hyper parameters\n",
    "    batch_size=128,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=15,\n",
    "    tolerance=1e-3,\n",
    "    seed=1337,\n",
    "    \n",
    "    # Runtime options\n",
    "    catch_keyboard_interrupt=True,\n",
    "    cuda=True if PYTORCH_DEVICE==\"cuda\" else False,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    pilot=PILOT_STUDY, # 2% of original dataset\n",
    "    train=TRAIN,\n",
    "    test_batch_size=int(2e3)\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "\n",
    "    args.model_state_file = os.path.join(args.model_save_dir,\n",
    "                                         args.model_state_file)\n",
    "    \n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "    \n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "utils.set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "utils.handle_dirs(args.model_save_dir)\n",
    "utils.handle_dirs(args.results_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5db264",
   "metadata": {},
   "source": [
    "# Hybrid CNN-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fc1ac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFHybrid(nn.Module):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        super(TFHybrid, self).__init__()\n",
    "        self.featurizer=models.TFCNN(channels=args.feat_size[0], \n",
    "                              conv_filters=args.conv_filters, conv_kernelsize=args.conv_kernelsize, \n",
    "                              maxpool_size=args.maxpool_size, maxpool_strides=args.maxpool_strides)\n",
    "        self.classifier=models.TFLSTM(input_features=args.conv_filters, lstm_nodes=args.lstm_outnodes, \n",
    "                               fc1_nodes=args.linear1_nodes)\n",
    "    \n",
    "        pass\n",
    "    \n",
    "    def forward(self, x_in, apply_sigmoid=False):\n",
    "        x_in = self.featurizer(x_in)\n",
    "        x_in = self.classifier(x_in)\n",
    "        \n",
    "        if apply_sigmoid:\n",
    "            x_in = torch.sigmoid(x_in)\n",
    "\n",
    "        return x_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8439a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hybrid(args):\n",
    "    \n",
    "    # Load the dataset\n",
    "    logging.debug(f'Loading dataset and creating vectorizer...')\n",
    "    dataset = datasets.TFDataset.load_dataset_and_vectorizer_from_path(args.source_csv, args.source_genome_fasta, ohe=True)    \n",
    "    \n",
    "    # Initializing model\n",
    "    logging.debug(f'Initializing model...')\n",
    "    classifier = TFHybrid(args)\n",
    "    classifier = classifier.to(args.device)\n",
    "    model_params = utils.get_n_params(classifier)\n",
    "    logging.debug(f\"The model has {model_params} parameters.\")\n",
    "        \n",
    "    # Defining loss function, optimizer and scheduler\n",
    "    loss_func = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate, eps=1e-7)\n",
    "    # adjusting the learning rate for better performance\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                     mode='min', factor=0.5,\n",
    "                                                     patience=1)    \n",
    "    \n",
    "    # Making samplers\n",
    "    train_sampler, valid_sampler = samplers.make_train_samplers(dataset, args)\n",
    "    logging.debug(f\"Training {model_params} parameters with {train_sampler.num_samples} instances at a rate of {round(train_sampler.num_samples/model_params, 6)} instances per parameter.\")\n",
    "    \n",
    "    # Defining initial train state\n",
    "    train_state = utils.make_train_state(args)\n",
    "    \n",
    "    # tqdm progress bar initialize\n",
    "    epoch_bar = tqdm.notebook.tqdm(desc='training routine', \n",
    "                          total=args.num_epochs,\n",
    "                          position=0)\n",
    "    \n",
    "    train_bar = tqdm.notebook.tqdm(desc=f'split=train',\n",
    "                              total=train_sampler.num_samples//args.batch_size, \n",
    "                              position=1, \n",
    "                              leave=True)\n",
    "    \n",
    "    dataset.set_split('valid')\n",
    "    val_bar = tqdm.notebook.tqdm(desc='split=valid',\n",
    "                        total=len(dataset)//int(args.batch_size*1e1), \n",
    "                        position=1, \n",
    "                        leave=True)\n",
    "    \n",
    "    ##### Training Routine #####\n",
    "    \n",
    "    try:\n",
    "        for epoch_index in range(args.num_epochs):\n",
    "            train_state['epoch_index'] = epoch_index\n",
    "\n",
    "            # Iterate over training dataset\n",
    "\n",
    "            # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "            dataset.set_split('train')\n",
    "            batch_generator = utils.generate_batches(dataset, sampler=train_sampler,\n",
    "                                               batch_size=args.batch_size, \n",
    "                                               device=args.device)\n",
    "            running_loss = 0.0\n",
    "            classifier.train()\n",
    "\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "                # the training routine as follows:\n",
    "\n",
    "                # --------------------------------------\n",
    "                # step 1. zero the gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # step 2. compute the output\n",
    "                y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "                # step 3. compute the loss\n",
    "                loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "\n",
    "                # step 4. use loss to produce gradients\n",
    "                loss.backward()\n",
    "\n",
    "                # step 5. use optimizer to take gradient step\n",
    "                optimizer.step()\n",
    "                # -----------------------------------------\n",
    "                # compute the loss for update\n",
    "                loss_t = loss.item()\n",
    "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "                # update bar\n",
    "                train_bar.set_postfix(loss=running_loss,\n",
    "                                      epoch=epoch_index)\n",
    "                train_bar.update()\n",
    "\n",
    "            train_state['train_loss'].append(running_loss)\n",
    "\n",
    "            # Iterate over val dataset\n",
    "\n",
    "            # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "            dataset.set_split('valid')\n",
    "            batch_generator = utils.generate_batches(dataset, sampler=valid_sampler,\n",
    "                                               batch_size=int(args.batch_size*1e1), \n",
    "                                               device=args.device)\n",
    "            running_loss = 0.\n",
    "            ## TODO::Calculate actual aps\n",
    "            tmp_filename = f\"./{TF}_hybrid_tmp.tmp\"\n",
    "            tmp_file = open(tmp_filename, \"wb\")\n",
    "            classifier.eval()\n",
    "\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "                # compute the output\n",
    "                y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "                y_target = batch_dict['y_target'].float()\n",
    "\n",
    "                # step 3. compute the loss\n",
    "                loss = loss_func(y_pred, y_target)\n",
    "                loss_t = loss.item()\n",
    "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "                # save data for computing aps\n",
    "                for yp, yt in zip(torch.sigmoid(y_pred).cpu().detach().numpy(), y_target.cpu().detach().numpy()):\n",
    "                    tmp_file.write(bytes(f\"{yp},{yt}\\n\", \"utf-8\"))\n",
    "\n",
    "                val_bar.set_postfix(loss=running_loss,\n",
    "                                    epoch=epoch_index,\n",
    "                                    early_stop=train_state['early_stopping_step'])\n",
    "                val_bar.update()\n",
    "\n",
    "            train_state['val_loss'].append(running_loss)\n",
    "            \n",
    "            # compute aps from saved file\n",
    "            tmp_file.close()\n",
    "            val_aps = utils.compute_aps_from_file(tmp_filename)\n",
    "            os.remove(tmp_filename)\n",
    "        \n",
    "            train_state['val_aps'].append(val_aps)\n",
    "\n",
    "            train_state = utils.update_train_state(args=args, model=classifier,\n",
    "                                             train_state=train_state)\n",
    "\n",
    "            scheduler.step(train_state['val_loss'][-1])\n",
    "            \n",
    "            logging.debug(f\"Epoch: {epoch_index}, Validation Loss: {running_loss}, Validation APS: {val_aps}\")\n",
    "\n",
    "            train_bar.n = 0\n",
    "            val_bar.n = 0\n",
    "            epoch_bar.update()\n",
    "\n",
    "            if train_state['stop_early']:\n",
    "                logging.debug(\"Early stopping criterion fulfilled!\")\n",
    "                break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        logging.warning(\"Exiting loop\")\n",
    "    \n",
    "    return train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b8b39bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a66c6e852cd447f1a33273757aaf2f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ba76afe69374556804d03b7ff0a5832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/9644 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e26bf6103e4c32b3bf2242f5ba66b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=valid:   0%|          | 0/2645 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/d/dzb5732/work/.dda/lib/python3.7/site-packages/torch/nn/modules/conv.py:295: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448224956/work/aten/src/ATen/native/Convolution.cpp:660.)\n",
      "  self.padding, self.dilation, self.groups)\n",
      "/storage/home/d/dzb5732/work/.dda/lib/python3.7/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448224956/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_state = train_hybrid(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b947f114",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca53889",
   "metadata": {},
   "source": [
    "## Source dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd7410b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = TFHybrid(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4c94a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dataset = datasets.TFDataset.load_dataset_and_vectorizer_from_path(args.source_csv, \n",
    "                                                                          args.source_genome_fasta, \n",
    "                                                                          ohe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5ccb606",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12fa1c6b6b0425daf0eac151f64f9d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=test:   0%|          | 0/1582 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/d/dzb5732/work/.dda/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:681: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../results/mm10/CEBPA/hybrid_src.csv.gz'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.eval_model(classifier, source_dataset, args, dataset_type=\"src\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69339f60",
   "metadata": {},
   "source": [
    "## Target dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f35434da",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dataset = datasets.TFDataset.load_dataset_and_vectorizer_from_path(args.target_csv, \n",
    "                                                                 args.target_genome_fasta, \n",
    "                                                                 ohe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cfd81f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d52681a04e6467291cb16c1fdfd0659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=test:   0%|          | 0/2169 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'../results/mm10/CEBPA/hybrid_tgt.csv.gz'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.eval_model(classifier, target_dataset, args, dataset_type=\"tgt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd2dcae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
