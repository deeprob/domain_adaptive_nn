{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ac2331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Function\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "from argparse import Namespace\n",
    "import tqdm\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61afbad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GLOBALS \n",
    "SOURCE_GENOME=\"mm10\"\n",
    "TF=\"CEBPA\"\n",
    "SOURCE_GENOME_FASTA='../../genomes/mm10_no_alt_analysis_set_ENCODE.fasta'\n",
    "TARGET_GENOME = \"hg38\"\n",
    "TARGET_GENOME_FASTA = \"../../genomes/GRCh38_no_alt_analysis_set_GCA_000001405.15.fasta\"\n",
    "PILOT_STUDY=False\n",
    "MODEL_NAME=\"dann\"\n",
    "PYTORCH_DEVICE=\"cuda\"\n",
    "TRAIN=True\n",
    "MODEL_STORAGE_SUFFIX=\"_pilot\" if PILOT_STUDY else \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784b6a0f",
   "metadata": {},
   "source": [
    "# Genome Dataset, Sampler and Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3df9947",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../\")\n",
    "from utils import datasets,samplers,models,utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc022584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logger config\n",
    "logging.basicConfig(filename=f'./log/{TF}_{MODEL_NAME}{MODEL_STORAGE_SUFFIX}.log', filemode='w', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475779e0",
   "metadata": {},
   "source": [
    "# Define namespace arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49144622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\t../torch_models/mm10/CEBPA/dann/dann.pth\n",
      "Using CUDA: True\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path information\n",
    "    model_state_file=f'{MODEL_NAME}{MODEL_STORAGE_SUFFIX}.pth',\n",
    "    source_csv=f'../data/{SOURCE_GENOME}/{TF}/split_data.csv.gz',\n",
    "    source_genome_fasta=SOURCE_GENOME_FASTA,\n",
    "    target_csv = f'../data/{TARGET_GENOME}/{TF}/split_data.csv.gz',\n",
    "    target_genome_fasta = TARGET_GENOME_FASTA,\n",
    "    model_save_dir=f'../torch_models/{SOURCE_GENOME}/{TF}/{MODEL_NAME}/',\n",
    "    results_save_dir=f'../results/{SOURCE_GENOME}/{TF}/',\n",
    "    feat_size=(4, 500),\n",
    "    \n",
    "    # Model hyper parameters\n",
    "    conv_filters=240,\n",
    "    conv_kernelsize=20,\n",
    "    maxpool_strides=15,\n",
    "    maxpool_size=15,\n",
    "    lstm_outnodes=32,\n",
    "    linear1_nodes=1024,\n",
    "    dropout_prob=0.5,\n",
    "    \n",
    "    # Training hyper parameters\n",
    "    batch_size=128,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=15,\n",
    "    tolerance=1e-3,\n",
    "    seed=1337,\n",
    "    \n",
    "    # Runtime options\n",
    "    catch_keyboard_interrupt=True,\n",
    "    cuda=True if PYTORCH_DEVICE==\"cuda\" else False,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    pilot=PILOT_STUDY, # 2% of original dataset\n",
    "    train=TRAIN,\n",
    "    test_batch_size=int(2e3)\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "\n",
    "    args.model_state_file = os.path.join(args.model_save_dir,\n",
    "                                         args.model_state_file)\n",
    "    \n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "    \n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "utils.set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "utils.handle_dirs(args.model_save_dir)\n",
    "utils.handle_dirs(args.results_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e80b2af",
   "metadata": {},
   "source": [
    "## DANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc170ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRL(Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_):\n",
    "        ctx.lambda_ = lambda_\n",
    "        return x.view_as(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.neg() * ctx.lambda_, None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a43e2ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFDANN(nn.Module):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        super(TFDANN, self).__init__()\n",
    "        \n",
    "        # featurizer\n",
    "        self.featurizer = models.TFCNN(channels=args.feat_size[0],\n",
    "                                       conv_filters=args.conv_filters, \n",
    "                                       conv_kernelsize=args.conv_kernelsize, \n",
    "                                       maxpool_size=args.maxpool_size, \n",
    "                                       maxpool_strides=args.maxpool_strides)\n",
    "\n",
    "        \n",
    "        # main classifier\n",
    "        self.classifier = models.TFLSTM(input_features=args.conv_filters, \n",
    "                                 lstm_nodes=args.lstm_outnodes, \n",
    "                                 fc1_nodes=args.linear1_nodes)\n",
    "        \n",
    "        linear_layer_in = int(np.floor((args.feat_size[1] - args.maxpool_size - 2)/args.maxpool_strides + 1)*args.conv_filters)\n",
    "\n",
    "        # domain classifier\n",
    "        self.discriminator = models.TFMLP(input_features=linear_layer_in, \n",
    "                                   fc1_nodes=args.linear1_nodes, \n",
    "                                   dropout_prob=0)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def forward(self, x_in, lambda_=1):\n",
    "        # Featurize\n",
    "        feature = self.featurizer(x_in)\n",
    "        \n",
    "        # feature transforms for different classifiers\n",
    "        dc_in = GRL.apply(feature, lambda_)\n",
    "        \n",
    "        # main classifier pipeline\n",
    "        binding_out = self.classifier(feature)\n",
    "        \n",
    "        # domain classifier pipeline\n",
    "        domain_out = self.discriminator(dc_in)\n",
    "        \n",
    "        return binding_out, domain_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b709c54c",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9955f0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dann(args):\n",
    "    \n",
    "    logging.debug(\"loading dataset...\")\n",
    "    dataset, target_dataset = datasets.load_data(args)\n",
    "        \n",
    "    # Initializing model\n",
    "    logging.debug(f'Initializing model...')\n",
    "    classifier = TFDANN(args)    \n",
    "    classifier = classifier.to(args.device)\n",
    "    model_params = utils.get_n_params(classifier)\n",
    "    logging.debug(f\"The model has {model_params} parameters.\")\n",
    "    \n",
    "    # Defining loss function, optimizer and scheduler\n",
    "    loss_func = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate, eps=1e-7)\n",
    "    # adjusting the learning rate for better performance\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                     mode='min', factor=0.5,\n",
    "                                                     patience=1)    \n",
    "    \n",
    "    # Making samplers\n",
    "    train_sampler, valid_sampler = samplers.make_train_samplers(dataset, args)\n",
    "    tr_samples = train_sampler.num_samples\n",
    "    dataset.set_split(\"valid\")\n",
    "    va_samples = len(dataset)\n",
    "    \n",
    "    # samplers for source and target data\n",
    "    dataset.set_split(\"train\")\n",
    "    source_sampler = samplers.get_sampler(dataset, weighted=False, mini=args.pilot)\n",
    "    target_dataset.set_split(\"train\")\n",
    "    target_sampler = samplers.get_sampler(target_dataset, weighted=False, mini=args.pilot)\n",
    "    \n",
    "    logging.debug(f\"Training {model_params} parameters with {tr_samples} instances at a rate of {round(tr_samples/model_params, 6)} instances per parameter.\")\n",
    "    \n",
    "    # Defining initial train state\n",
    "    train_state = utils.make_train_state(args)\n",
    "    \n",
    "    # tqdm progress bars initialize\n",
    "    epoch_bar = tqdm.notebook.tqdm(desc='training routine', \n",
    "                          total=args.num_epochs,\n",
    "                          position=0)\n",
    "    \n",
    "    train_bar = tqdm.notebook.tqdm(desc=f'split=train',\n",
    "                              total=tr_samples//args.batch_size, \n",
    "                              position=1, \n",
    "                              leave=True)\n",
    "    \n",
    "    val_bar = tqdm.notebook.tqdm(desc='split=valid',\n",
    "                        total=va_samples//int(args.batch_size*1e1), \n",
    "                        position=1, \n",
    "                        leave=True)\n",
    "\n",
    "    ##### Training Routine #####\n",
    "    \n",
    "    try:\n",
    "        for epoch_index in range(args.num_epochs):\n",
    "            train_state['epoch_index'] = epoch_index\n",
    "\n",
    "            # Iterate over training dataset\n",
    "\n",
    "            # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "            dataset.set_split('train')\n",
    "            target_dataset.set_split('train')\n",
    "            batch_generator = utils.generate_batches(dataset, sampler=train_sampler,\n",
    "                                               batch_size=args.batch_size, \n",
    "                                               device=args.device)\n",
    "            \n",
    "            source_batch_generator = utils.generate_batches(dataset, sampler=source_sampler,\n",
    "                                   batch_size=args.batch_size, \n",
    "                                   device=args.device)\n",
    "            \n",
    "            target_batch_generator = utils.generate_batches(target_dataset, sampler=target_sampler,\n",
    "                                               batch_size=args.batch_size, \n",
    "                                               device=args.device)\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_domainacc = 0.0\n",
    "            classifier.train()\n",
    "            \n",
    "            for batch_index, (batch_dict, source_batch_dict, target_batch_dict) in enumerate(zip(batch_generator, source_batch_generator, target_batch_generator)):\n",
    "                \n",
    "                # the training routine is these 8 steps:\n",
    "\n",
    "                # --------------------------------------\n",
    "                # step 1. zero the gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # step 2. compute the output with balanced data\n",
    "                y_pred, _ = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "                # step 3. compute the source classifier loss with balanced data\n",
    "                loss_class = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "                \n",
    "                # step 4. compute domain loss with random data from source and target species\n",
    "                domain_in = torch.cat((source_batch_dict['x_data'].float(),\n",
    "                                       target_batch_dict['x_data'].float()))\n",
    "                \n",
    "                \n",
    "                domain_label = torch.cat((torch.zeros(args.batch_size, dtype=torch.float, device=args.device),\n",
    "                                          torch.ones(args.batch_size, dtype=torch.float, device=args.device)))\n",
    "                \n",
    "                _, domain_pred = classifier(x_in=domain_in)\n",
    "                loss_domain = loss_func(domain_pred, domain_label)\n",
    "                                \n",
    "                # step 5. use losses to produce gradients\n",
    "                loss = loss_class + loss_domain\n",
    "                loss.backward()\n",
    "\n",
    "                # step 6. use optimizer to take gradient step\n",
    "                optimizer.step()\n",
    "                # -----------------------------------------\n",
    "                \n",
    "                # compute the average precision score\n",
    "                loss_t = loss_class.item()\n",
    "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "                \n",
    "                domain_hat = domain_pred>0.5\n",
    "                domain_hat = domain_hat.long()\n",
    "                acc_domain = torch.sum(domain_hat==domain_label)/len(domain_label)\n",
    "                acc_domain = acc_domain.item()\n",
    "                running_domainacc += (acc_domain - running_domainacc) / (batch_index + 1)\n",
    "\n",
    "                # update bar\n",
    "                train_bar.set_postfix(loss=running_loss,\n",
    "                                      dacc=running_domainacc,\n",
    "                                      epoch=epoch_index)\n",
    "                train_bar.update()\n",
    "\n",
    "            train_state['train_loss'].append(running_loss)\n",
    "\n",
    "            # Iterate over val dataset\n",
    "\n",
    "            # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "            dataset.set_split('valid')\n",
    "            batch_generator = utils.generate_batches(dataset, sampler=valid_sampler,\n",
    "                                               batch_size=int(args.batch_size*1e1), \n",
    "                                               device=args.device)\n",
    "            running_loss = 0.\n",
    "            tmp_filename = f\"./{TF}_dann_tmp.tmp\"\n",
    "            tmp_file = open(tmp_filename, \"wb\")\n",
    "            classifier.eval()\n",
    "\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "                # compute the output\n",
    "                y_pred, _ = classifier(x_in=batch_dict['x_data'].float())\n",
    "                y_target = batch_dict['y_target'].float()\n",
    "\n",
    "                # step 3. compute the loss\n",
    "                loss = loss_func(y_pred, y_target)\n",
    "                loss_t = loss.item()\n",
    "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "                # compute aps\n",
    "                for yp, yt in zip(torch.sigmoid(y_pred).cpu().detach().numpy(), y_target.cpu().detach().numpy()):\n",
    "                    tmp_file.write(bytes(f\"{yp},{yt}\\n\", \"utf-8\"))\n",
    "\n",
    "                val_bar.set_postfix(loss=running_loss, \n",
    "                                    epoch=epoch_index,\n",
    "                                    early_stop=train_state['early_stopping_step'])\n",
    "                val_bar.update()\n",
    "\n",
    "            train_state['val_loss'].append(running_loss)\n",
    "            # compute aps from saved file\n",
    "            tmp_file.close()\n",
    "            val_aps = utils.compute_aps_from_file(tmp_filename)\n",
    "            os.remove(tmp_filename)\n",
    "            \n",
    "            train_state['val_aps'].append(val_aps)\n",
    "\n",
    "            train_state = utils.update_train_state(args=args, model=classifier,\n",
    "                                             train_state=train_state)\n",
    "\n",
    "            scheduler.step(train_state['val_loss'][-1])\n",
    "            \n",
    "            logging.debug(f\"Epoch: {epoch_index}, Validation Loss: {running_loss}, Validation APS: {val_aps}\")\n",
    "\n",
    "            train_bar.n = 0\n",
    "            val_bar.n = 0\n",
    "            epoch_bar.update()\n",
    "\n",
    "            if train_state['stop_early']:\n",
    "                logging.debug(\"Early stopping criterion fulfilled!\")\n",
    "                break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        logging.warning(\"Exiting loop\")\n",
    "        \n",
    "    return train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8167d41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4770ac82cfee439eaa52a30cbbee132d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "563f0c6acc0c4f83adced80beee81183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/9644 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c33b6af444451caf1e1c234e851c7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=valid:   0%|          | 0/2645 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/d/dzb5732/work/.dda/lib/python3.7/site-packages/torch/nn/modules/conv.py:295: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448224956/work/aten/src/ATen/native/Convolution.cpp:660.)\n",
      "  self.padding, self.dilation, self.groups)\n",
      "/storage/home/d/dzb5732/work/.dda/lib/python3.7/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448224956/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    info = train_dann(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b947f114",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca53889",
   "metadata": {},
   "source": [
    "## Source dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152ed182",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = TFDANN(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4503126",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dataset = datasets.TFDataset.load_dataset_and_vectorizer_from_path(args.source_csv, \n",
    "                                                                 args.source_genome_fasta, \n",
    "                                                                 ohe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ccb606",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "utils.eval_model(classifier, source_dataset, args, dataset_type=\"src\", model=\"dann\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69339f60",
   "metadata": {},
   "source": [
    "## Target dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35434da",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dataset = datasets.TFDataset.load_dataset_and_vectorizer_from_path(args.target_csv, \n",
    "                                                                 args.target_genome_fasta, \n",
    "                                                                 ohe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfd81f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "utils.eval_model(classifier, target_dataset, args, dataset_type=\"tgt\", model=\"dann\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5a5d60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
