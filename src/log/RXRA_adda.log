INFO:root:Expanded filepaths: 
INFO:root:	../torch_models/mm10/RXRA/adda/adda.pth
INFO:root:Using CUDA: True
DEBUG:root:==== Loading model for source domain ====
DEBUG:root:>>> Source Encoder <<<
DEBUG:root:TFCNN(
  (featurizer): Sequential(
    (0): Conv1d(4, 240, kernel_size=(20,), stride=(1,), padding=same)
    (1): ReLU()
    (2): MaxPool1d(kernel_size=15, stride=15, padding=0, dilation=1, ceil_mode=False)
  )
)
DEBUG:root:>>> Source Classifier <<<
DEBUG:root:TFLSTM(
  (lstm): LSTM(240, 32, batch_first=True)
  (fclayers): TFMLP(
    (fclayers): Sequential(
      (0): Linear(in_features=32, out_features=1024, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.5, inplace=False)
      (3): Linear(in_features=1024, out_features=512, bias=True)
      (4): Sigmoid()
      (5): Linear(in_features=512, out_features=1, bias=True)
    )
  )
)
DEBUG:root:==== Training encoder for target domain ====
DEBUG:root:>>> Target Encoder <<<
DEBUG:root:TFCNN(
  (featurizer): Sequential(
    (0): Conv1d(4, 240, kernel_size=(20,), stride=(1,), padding=same)
    (1): ReLU()
    (2): MaxPool1d(kernel_size=15, stride=15, padding=0, dilation=1, ceil_mode=False)
  )
)
DEBUG:root:>>> Discriminator <<<
DEBUG:root:TFMLP(
  (fclayers): Sequential(
    (0): Linear(in_features=7920, out_features=1024, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=1024, out_features=512, bias=True)
    (4): Sigmoid()
    (5): Linear(in_features=512, out_features=1, bias=True)
  )
)
DEBUG:root:Source samples: 377380, Target samples: 431797
DEBUG:root:DA: 0.49080424955412405, Tgt Enc Loss: 0.6950393407283356, DSCM Loss: 0.6947637879956261
DEBUG:root:DA: 0.5529287665196354, Tgt Enc Loss: 0.6934447654875511, DSCM Loss: 0.692648900137948
DEBUG:root:DA: 0.4824045556221589, Tgt Enc Loss: 0.6933554457033022, DSCM Loss: 0.6934631615623089
DEBUG:root:DA: 0.45403830180297144, Tgt Enc Loss: 0.693475596455091, DSCM Loss: 0.6934966894114621
DEBUG:root:DA: 0.46998476226636343, Tgt Enc Loss: 0.6936428716742364, DSCM Loss: 0.6934187050112168
